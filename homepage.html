<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  </script>

  <title>Hossein kashiani</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="png" href="Doc/icon.png">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:6%;width:63%;text-align: justify">
              <p style="text-align:center">
                <name>Hossein Kashiani</name>
              </p>
			  <p>I am a research assistant at the Computer Vision Center at the Iran University of Science & Technology. I received my master's degree in Electrical Engineering in 2018, supervised by Prof. <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar Baradaran Shokouhi</a>. My research interests lie in computer vision, object tracking, and object classification. Much of my current research is about developing a robust visual object tracking algorithm for complicated scenarios using Convolutional Neural Networks.
              </p>

              </p>
              <p style="text-align:center">

				<a href="mailto:kashianihossein@gmail.com">Email</a> &nbsp/&nbsp
                <!--<a href="Doc/homepage_cv.pdf">CV</a> &nbsp/&nbsp-->
                <a href="https://www.linkedin.com/in/hossein-kashiani/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://twitter.com/Hossein_ksh"> Twitter </a>
              </p>
            </td>
            <td style="padding:2.5%;width:20%;max-width:20%">
			  <img width=100% src="Doc/circle-cropped.png">

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:30px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="width:100%;">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:120%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image'><img src='Doc/IMAVIS_2.png'></div>
                <img src='Doc/IMAVIS_1.png'>

              </div>
              <script type="text/javascript">
                function nightsight_start() {
                  document.getElementById('nightsight_image').style.opacity = "1";
                }
                function nightsight_stop() {
                  document.getElementById('nightsight_image').style.opacity = "0";
                }
                nightsight_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">
			  <a href="https://www.sciencedirect.com/science/article/pii/S0262885619300113">
                <papertitle> Visual object tracking based on adaptive siamese and motion estimation network.</papertitle>
              </a>
              <br>
              <strong>Hossein Kashiani</strong>,
			  <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B Shokouhi</a>
              <br>
			  <em>Image and Vision Computing</em>, 2019
              <br>
			  [<a href="Doc/Visual Object Tracking based on Adaptive Siamese and Motion Estimation Network(revised).pdf">PDF</a>]
               <a href="Doc/2018.bib">[Bibtex]</a> 
               <!--[<a href="https://www.youtube.com/watch?v=ZYtsb9-1zXk&feature=youtu.be">Video</a>]     -->
			   <br>
              <p></p>
              <p>In this work, we aim to improve both the motion and observation models in visual object tracking by leveraging the
				representation power of CNNs. To this end, a motion estimation network (named MEN) is utilized to seek
				the most likely locations of the target and prepare a further clue in addition to the previous target position.
				Hence the motion estimation would be enhanced by generating a small number of candidates near two
				plausible positions. The generated candidates are then fed into a trained Siamese network to detect the
				most probable candidate. Each candidate is compared to an adaptable buffer, which is updated under a
				predefined condition. To take into account the target appearance changes, a weighting CNN (called WCNN)
				adaptively assigns weights to the final similarity scores of the Siamese network using sequence-specific
				information.</p>
            </td>
          </tr>
          
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='font_image'><img src='Doc/ICCKE_2.png'></div>
                <img src='Doc/both_skaters_ICCKE.gif'>
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }
                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">
              <a href="https://ieeexplore.ieee.org/document/8167940">
                <papertitle>Patchwise object tracking via structural local sparse appearance model.</papertitle>
              </a>
              <br>
              <strong>Hossein Kashiani</strong>,
			  <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B Shokouhi</a>
              <br>
			  <em>International Conference on Computer and Knowledge Engineering </em>, 2017
              <br>
			   [<a href="https://arxiv.org/abs/1803.06141">ArXiv</a>]
               <a href="Doc/2018.bib">[Bibtex]</a> 
               [<a href="Doc/ICCKE.pptx">Slide</a>]
			   <br>
              <p></p>
              <p>In this paper, we propose a robust visual tracking method which exploits the relationships of targets in adjacent
				  frames using patchwise joint sparse representation. Two sets of overlapping patches with different sizes are extracted from target
				  candidates to construct two dictionaries with consideration of joint
				  sparse representation. By applying this representation into structural sparse appearance model, we can take two-fold advantages.
				  First, the correlation of target patches over time is considered.
				  Second, using this local appearance model with different patch
				  sizes takes into account local features of target thoroughly.
				  Furthermore, the position of candidate patches and their occlusion
				  levels are utilized simultaneously to obtain the final likelihood of
				  target candidates. Evaluations on recent challenging benchmark
				  show that our tracking method outperforms the state-of-the-art trackers.</p>
            </td>
          </tr>



		<table width="100%" align="center" border="0" cellpadding="20">
		<tr><td>
			<heading>Teaching Assistant</heading>
			<ul>
			<li> Computer Vision</li>
			<li> Intelligent systems</li>
			</ul>
		  </td></tr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
		      Time saved by <a href="https://jonbarron.info//">this</a> awesome website.
	    </font>
        </p>
        </td>
        </tr>
		
      </td>
    </tr>
  </table>
</body>

</html>