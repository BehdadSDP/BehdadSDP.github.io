<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Academic portfolio of Behdad Sadeghian Pour, MSc student at Iran University of Science & Technology">
  <title>Behdad Sadeghian Pour | Academic Portfolio</title>
  
  <link rel="stylesheet" href="stylesheet.css">
  <link rel="icon" type="image/png" href="Doc/icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>

<body>
  <nav class="navbar">
    <div class="nav-container">
      <div class="nav-logo">BS</div>
      <ul class="nav-menu">
        <li><a href="#about">About</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#projects">Projects</a></li>
        <li><a href="#teaching">Teaching</a></li>
        <li><a href="#contact">Contact</a></li>
        <li><a href="cv.html">CV</a></li>
      </ul>
    </div>
  </nav>

  <main>
    <section id="about" class="hero-section">
      <div class="profile-container">
        <img src="Doc/circle-cropped.png" alt="Behdad Sadeghian Pour's profile picture" class="profile-image">
        <div class="profile-info">
          <h1>Behdad Sadeghian Pour</h1>
          <p class="subtitle">MSc Student, Computer Vision Research</p>
          <p>I am a Master of Science student at the Iran University of Science & Technology, supervised by Prof. <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar Baradaran Shokouhi</a>. My research focuses on computer vision with applications in adverse weather conditions and human action recognition.</p>
          <div class="social-links" id="contact">
            <a href="mailto:p.sadeghian.p@gmail.com" aria-label="Email"><i class="fas fa-envelope"></i> Email</a>
            <!--<a href="Doc/homepage_cv.pdf"><i class="fas fa-file-pdf"></i> CV</a>-->
            <a href="https://www.linkedin.com/in/behdad-sadeghian-pour-911444215/" aria-label="LinkedIn"><i class="fab fa-linkedin"></i> LinkedIn</a>
            <a href="https://github.com/BehdadSDP" aria-label="GitHub"><i class="fab fa-github"></i> GitHub</a>
            <a href="https://scholar.google.com/citations?user=your-id" aria-label="Google Scholar"><i class="fas fa-graduation-cap"></i> Scholar</a>
          </div>
        </div>
      </div>
    </section>

    <section id="publications" class="publications-section">
      <h2>Publications</h2>
      
      <div class="publication-category">
        <h3 class="category-title">Peer-Reviewed Conference Papers</h3>
        
        <article class="publication-card">
          <div class="publication-image">
            <img src="Doc/1.png" alt="AL-YOLO paper image">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">AL-YOLO: Accurate and Lightweight Vehicle and Pedestrian Detector in Foggy Weather</h3>
            <p class="publication-authors"><strong>Behdad Sadeghian Pour</strong>, Hamidreza Mohammadi Jozani, <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B. Shokouhi</a></p>
            <p class="publication-venue"><em>The 14th International Conference on Computer and Knowledge Engineering (ICCKE 2024)</em></p>
            <div class="publication-links">
              <a href="https://github.com/BehdadSDP/AL-YOLO" class="button"><i class="fab fa-github"></i> Code</a>
              <a href="https://ieeexplore.ieee.org/document/10874685" class="button"><i class="fas fa-external-link-alt"></i> IEEE Xplore</a>
              <button class="button" onclick="toggleAbstract('abstract1')"><i class="fas fa-align-left"></i> Abstract</button>
            </div>
            <div id="abstract1" class="abstract">
              <p>One of the most critical tasks in computer vision is object detection. While object detection networks have demonstrated high accuracy in normal weather conditions, they are not reliable in adverse weather. To address this issue, we have enhanced a smaller version of YOLOv5, known as YOLOv5s, to be compatible with challenging weather conditions and devices with limited resources. As a result, we modified the YOLOv5s architecture to extract significant features. We used a lightweight transformer network known as MobileViTv2 with an Inverted Residual Block, which is efficient in terms of computational resources. Furthermore, the YOLOv5 Neck has incorporated the C3RFE module to improve feature extraction efficiency in adverse weather. In order to assess the effectiveness of our suggested method, we conducted a thorough evaluation using a Foggy-Cityscape dataset. The results demonstrate that in comparison to the YOLOV5s, the algorithm has a 33% decrease in the number of model parameters and also increases by about 2% in mAP. Comparative analysis demonstrates the algorithm's superiority and effectiveness.</p>
            </div>
          </div>
        </article>

        <article class="publication-card">
          <div class="publication-image">
            <img src="Doc/3.png" alt="CNN and ViT paper image">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">CNN and ViT as Dual Teachers for Knowledge Distillation in Human Action Recognition</h3>
            <p class="publication-authors">Hamidreza Mohammadi Jozani, <strong>Behdad Sadeghian Pour</strong>, <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B. Shokouhi</a></p>
            <p class="publication-venue"><em>The 10th International Conference on Signal Processing and Intelligent Systems (ICSPIS 2024)</em></p>
            <div class="publication-links">
              <a href="https://ieeexplore.ieee.org/document/10931096" class="button"><i class="fas fa-external-link-alt"></i> IEEE Xplore</a>
              <button class="button" onclick="toggleAbstract('abstract2')"><i class="fas fa-align-left"></i> Abstract</button>
            </div>
            <div id="abstract2" class="abstract">
              <p>Recognizing actions in still images remains difficult, even if Convolutional Neural Networks (CNNs) have made significant advancements in picture categorization. The main difficulty in recognizing human action from photographs is the lack of visual indications of temporal information in static photos. Presently, the most effective approaches involve training a deep Convolutional Neural Network directly on images for action recognition. Nevertheless, these approaches include numerous parameters and incur substantial computational expenses. Moreover, the majority of existing techniques have utilized supplementary information, such as human body movements, relevant objects, and the visual characteristics of body parts, obtained from photos. These approaches utilize object detection or pose estimation as an auxiliary method in both the training and testing phases. Creating these annotations is a laborious and expensive process. In this paper, we utilize a convolutional neural network (CNN) as the student model, whereas a combination of a larger convolutional neural network (CNN) and a vision transformer (ViT) function as the teacher model. The CNN teacher model collects local picture features, while the ViT teacher emphasizes global features through an attention mechanism. Both local and global features can be advantageous in human action recognition; therefore, we attempt to integrate ViT and CNN teachers to develop the optimal teacher for the student model. This is achieved through a loss-aware module, which adaptively allocates sample-wise reliability to each teacher's prediction based on ground-truth labels, assigning greater weights to predictions that closely match one-hot labels. Our method achieves a mean Average Precision (mAP) of 95.17% on the Stanford40 dataset, which is the highest performance.</p>
            </div>
          </div>
        </article>
      </div>

      <div class="publication-category">
        <h3 class="category-title">Preprints</h3>
        
        <article class="publication-card">
          <div class="publication-image">
            <img src="Doc/2.png" alt="Knowledge Distillation paper image">
          </div>
          <div class="publication-content">
            <h3 class="publication-title">Knowledge Distillation with the Reused Teacher Classifier Framework for Action Recognition in Still Images</h3>
            <p class="publication-authors">Hamidreza Mohammadi Jozani, <strong>Behdad Sadeghian Pour</strong>, <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B. Shokouhi</a></p>
            <div class="publication-links">
              <button class="button" onclick="toggleAbstract('abstract3')"><i class="fas fa-align-left"></i> Abstract</button>
            </div>
            <div id="abstract3" class="abstract">
              <p>Recognizing actions in still images remains difficult, even if Convolutional Neural Networks (CNNs) have made significant advancements in picture categorization. The main difficulty in recognizing human action from photographs is the lack of visual indications of temporal information in static photos. Presently, the most effective approaches involve training a deep Convolutional Neural Network directly on images for action recognition. Nevertheless, these approaches include numerous parameters and incur substantial computational expenses. Moreover, the majority of existing techniques have utilized supplementary information, such as human body movements, relevant objects, and the visual characteristics of body parts, obtained from photos. These approaches utilize object detection or pose estimation as an auxiliary method in both the training and testing phases. Creating these annotations is a laborious and expensive process. In this paper, we propose a knowledge distillation framework to enhance the process of recognizing actions in still images. This framework uses a technique that employs the discriminative classifier from the pre-trained teacher model for direct usage in student inference. Additionally, we train a student encoder by aligning features using a single â„“2 loss. Also, self-distillation is employed to enhance the performance of instructor networks, hence leading to an increase in the accuracy of student networks. Our framework achieves a mean Average Precision (mAP) of 96.54% on the Stanford40 dataset, which is the highest performance.</p>
            </div>
          </div>
        </article>
      </div>
    </section>

    <section id="projects" class="projects-section">
      <h2>Projects</h2>
      <div class="projects-container">
        <article class="project-card">
          <div class="project-content">
            <h3 class="project-title">AL-YOLO: Accurate and Lightweight Vehicle and Pedestrian Detector</h3>
            <p class="project-meta">Python | Deep Learning | 2024</p>
            <p class="project-description">Developed AL-YOLO, an enhanced YOLOv5-based object detection model optimized for foggy weather conditions and resource-constrained devices. The architecture incorporates MobileViTv2 with Inverted Residual Blocks for efficient feature extraction and a C3RFE module in the neck for improved performance in adverse weather. Achieved a 33% reduction in model parameters while increasing mAP by approximately 2% compared to YOLOv5s on the Foggy-Cityscape dataset.</p>
            <div class="project-tags">
              <span class="project-tag">YOLOv5</span>
              <span class="project-tag">Object Detection</span>
              <span class="project-tag">MobileViT</span>
              <span class="project-tag">Foggy Weather</span>
              <span class="project-tag">Computer Vision</span>
            </div>
            <div class="project-links">
              <a href="https://github.com/BehdadSDP/AL-YOLO" class="button"><i class="fab fa-github"></i> Code</a>
              <a href="https://ieeexplore.ieee.org/document/10874685" class="button"><i class="fas fa-external-link-alt"></i> IEEE Xplore</a>
            </div>
          </div>
        </article>

        <article class="project-card">
          <div class="project-content">
            <h3 class="project-title">ADF4360-5 SPI Controller Implementation</h3>
            <p class="project-meta">VHDL | SPI Protocol | 2022</p>
            <p class="project-description">Designed and implemented a VHDL-based SPI controller for the ADF4360-5 Integrated Synthesizer and VCO. The project involved simulating the data transfer protocol according to the device datasheet, implementing the SPI communication interface, and documenting the digital architecture of the frequency synthesizer chip. This was completed as part of a VHDL course at Iran University of Science & Technology under Dr. Mirzakuchaki's supervision.</p>
            <div class="project-tags">
              <span class="project-tag">VHDL</span>
              <span class="project-tag">SPI</span>
              <span class="project-tag">FPGA</span>
              <span class="project-tag">Frequency Synthesizer</span>
            </div>
            <div class="project-links">
              <a href="https://github.com/Dr-Mirzakuchaki-IUST-TAs/401611184" class="button"><i class="fab fa-github"></i> Code</a>
            </div>
          </div>
        </article>
      </div>
    </section>

    <section id="teaching" class="teaching-section">
      <h2>Teaching Experience</h2>
      <div class="teaching-container">
        <div class="teaching-card">
          <div class="teaching-icon"><i class="fas fa-microchip"></i></div>
          <div class="teaching-details">
            <h3>Microprocessor Programming</h3>
            <p>Teaching Assistant, Undergraduate Course</p>
            <p class="teaching-date">Fall 2020</p>
          </div>
        </div>
        <div class="teaching-card">
          <div class="teaching-icon"><i class="fas fa-wave-square"></i></div>
          <div class="teaching-details">
            <h3>Digital Signal Processing</h3>
            <p>Teaching Assistant, Graduate Course</p>
            <p class="teaching-date">Winter 2023</p>
          </div>
        </div>
      </div>
    </section>
  </main>

  <footer>
    <div class="footer-content">
      <p>&copy; 2024 Behdad Sadeghian Pour. All rights reserved.</p>
      <p>Last updated: May 2024</p>
    </div>
  </footer>

  <script>
    function toggleAbstract(id) {
      const abstract = document.getElementById(id);
      if (abstract.style.display === "block") {
        abstract.style.display = "none";
      } else {
        abstract.style.display = "block";
      }
    }

    // Simple sticky navigation
    window.addEventListener('scroll', function() {
      const navbar = document.querySelector('.navbar');
      if (window.scrollY > 50) {
        navbar.classList.add('scrolled');
      } else {
        navbar.classList.remove('scrolled');
      }
    });
  </script>
</body>
</html>
